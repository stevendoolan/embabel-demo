# Configuration for Embabel to use Ollama local models.
#
# Install Ollama from https://ollama.com/
# Install the required LLMs onto Ollama:
#
#     ollama run gpt-oss
#     ollama run qwen3:0.6b
#

embabel.models.defaultLlm=gpt-oss:latest
embabel.models.llms.best=gpt-oss:latest
embabel.models.llms.cheapest=qwen3:0.6b

# Enable virtual threads for better concurrency with LLM calls
spring.threads.virtual.enabled=true